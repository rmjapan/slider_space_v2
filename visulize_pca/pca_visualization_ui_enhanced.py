#!/usr/bin/env python3
"""
æ‹¡å¼µç‰ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰PCAå¯è¦–åŒ–UI - Streamlitç‰ˆï¼ˆä¸»æˆåˆ†é¸æŠæ©Ÿèƒ½ä»˜ãï¼‰

æ©Ÿèƒ½:
1. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
2. æ•£å¸ƒå›³ä¸Šã«ç›´æ¥ç”»åƒè¡¨ç¤º
3. ä»»æ„ã®ä¸»æˆåˆ†çµ„ã¿åˆã‚ã›é¸æŠ
4. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–æ›´æ–°
5. çµæœã®ä¿å­˜ãƒ»è¡¨ç¤º
6. è¤‡æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼å¯¾å¿œ
7. é«˜è§£åƒåº¦å‡ºåŠ›
8. å¯„ä¸ç‡åˆ†æè¡¨ç¤º

UIç‰ˆã¨ç›´æ¥ç‰ˆã®è‰¯ã„ã¨ã“ã‚ã‚’çµ„åˆã‚ã›ãŸãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚·ã‚¹ãƒ†ãƒ  + ä¸»æˆåˆ†é¸æŠæ©Ÿèƒ½
"""

import streamlit as st
import os
import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from pathlib import Path
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from transformers import CLIPModel, CLIPProcessor, AutoModel
import torch.nn.functional as F
from typing import List, Optional, Tuple
from matplotlib.offsetbox import OffsetImage, AnnotationBbox
import random
import io
import base64
import time
import seaborn as sns
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from io import BytesIO

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œå‡º
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)  # visulize_pcaã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
OPENVISION_AVAILABLE = False
EMOTION_CLIP_AVAILABLE = False

try:
    from utils.model_util import load_openvision_model
    OPENVISION_AVAILABLE = True
except ImportError:
    pass

try:
    from EmotionCLIP.src.models.base import EmotionCLIP
    EMOTION_CLIP_AVAILABLE = True
except ImportError:
    pass

# Streamlitãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ğŸ¨ æ‹¡å¼µç‰ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰PCAå¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ¨",
    layout="wide",
    initial_sidebar_state="expanded"
)

@st.cache_data
def load_image_dataset(data_path: str, max_images: int) -> Tuple[List[Image.Image], List[str]]:
    """
    ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰
    """
    data_path = Path(data_path)
    images = []
    image_paths = []
    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
    
    if data_path.is_dir():
        for file_path in list(data_path.rglob('*'))[:max_images]:
            if file_path.suffix.lower() in image_extensions:
                try:
                    img = Image.open(file_path).convert('RGB')
                    images.append(img)
                    image_paths.append(str(file_path))
                except Exception:
                    continue
    
    return images, image_paths

@st.cache_resource
def load_encoder(encoder_type: str, device: str = 'cuda'):
    """
    ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’èª­ã¿è¾¼ã‚€ï¼ˆãƒªã‚½ãƒ¼ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰
    """
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‘ã‚¹ã‚’å–å¾—
    checkpoint_path = getattr(st.session_state, f'{encoder_type}_checkpoint', None)
    if not checkpoint_path:
        default_paths = get_default_checkpoint_paths()
        checkpoint_path = default_paths.get(encoder_type, "")
    
    if encoder_type == 'clip':
        try:
            model = CLIPModel.from_pretrained(
                checkpoint_path,
                torch_dtype=torch.float16
            )
            processor = CLIPProcessor.from_pretrained(checkpoint_path)
            st.info(f"âœ… CLIP ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {checkpoint_path}")
        except Exception as e:
            st.error(f"âŒ CLIP ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {str(e)}")
            raise
        
    elif encoder_type == 'emotion_clip':
        if not EMOTION_CLIP_AVAILABLE:
            raise ImportError("EmotionCLIPã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        if not os.path.exists(checkpoint_path):
            candidates = find_checkpoint_candidates('emotion_clip')
            error_msg = f"EmotionCLIPã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {checkpoint_path}"
            if candidates:
                error_msg += f"\n\nåˆ©ç”¨å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«:\n" + "\n".join(f"  â€¢ {p}" for p in candidates[:5])
                if len(candidates) > 5:
                    error_msg += f"\n  ... ä»– {len(candidates) - 5} å€‹"
            raise FileNotFoundError(error_msg)
        
        try:
            checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
            model = EmotionCLIP(video_len=8, backbone_checkpoint=None)
            model.load_state_dict(checkpoint['model'], strict=True)
            processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
            st.info(f"âœ… EmotionCLIP ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {Path(checkpoint_path).name}")
        except Exception as e:
            raise RuntimeError(f"EmotionCLIPãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {str(e)}")
        
    elif encoder_type == 'dinov2-small':
        try:
            model = AutoModel.from_pretrained(
                checkpoint_path,
                torch_dtype=torch.float16
            )
            processor = None
            st.info(f"âœ… DINOv2 ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {checkpoint_path}")
        except Exception as e:
            st.error(f"âŒ DINOv2 ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {str(e)}")
            raise
        
    elif encoder_type == 'openvision':
        if not OPENVISION_AVAILABLE:
            raise ImportError("OpenVisionã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        try:
            if os.path.exists(checkpoint_path):
                # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
                model = load_openvision_model(checkpoint_path)
                st.info(f"âœ… OpenVision ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {Path(checkpoint_path).name}")
            else:
                # Hugging Face IDã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ã§èª­ã¿è¾¼ã¿
                model = load_openvision_model(checkpoint_path)
                st.info(f"âœ… OpenVision ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {checkpoint_path}")
            processor = None
        except Exception as e:
            raise RuntimeError(f"OpenVisionãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {str(e)}")
        
    else:
        raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚¿ã‚¤ãƒ—: {encoder_type}")
    
    model.eval()
    model.to(device)
    if encoder_type == 'emotion_clip':
        model = model.to(torch.bfloat16)
    model.requires_grad_(False)
    
    return model, processor

@st.cache_data
def encode_images(images: List[Image.Image], encoder_type: str, batch_size: int = 8, device: str = 'cuda') -> np.ndarray:
    """
    ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰
    """
    model, processor = load_encoder(encoder_type, device)
    
    all_features = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i in range(0, len(images), batch_size):
        batch_images = images[i:i+batch_size]
        progress = (i + batch_size) / len(images)
        progress_bar.progress(min(progress, 1.0))
        status_text.text(f"ğŸ”„ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ä¸­: {i+1}-{min(i+batch_size, len(images))}/{len(images)}")
        
        with torch.no_grad():
            if encoder_type == 'clip':
                inputs = processor(images=batch_images, return_tensors="pt", padding=True)
                inputs = {k: v.to(device) for k, v in inputs.items()}
                outputs = model.get_image_features(**inputs)
                features = F.normalize(outputs, dim=-1)
                
            elif encoder_type == 'emotion_clip':
                clip_inputs = processor(images=batch_images, return_tensors="pt", padding=True)
                pixel_values = clip_inputs['pixel_values'].to(device)
                mask = torch.ones((len(batch_images), 224, 224), dtype=torch.bfloat16, device=device)
                features = model.encode_image(pixel_values.to(torch.bfloat16), mask)
                features = F.normalize(features, dim=-1)
                
            elif encoder_type == 'dinov2-small':
                from torchvision import transforms
                transform = transforms.Compose([
                    transforms.Resize((224, 224)),
                    transforms.ToTensor(),
                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
                ])
                
                processed_images = []
                for img in batch_images:
                    processed_img = transform(img)
                    processed_images.append(processed_img)
                
                batch_tensor = torch.stack(processed_images).to(device)
                outputs = model(batch_tensor)
                features = outputs.last_hidden_state[:, 0, :]
                features = F.normalize(features, dim=-1)
            
            all_features.append(features.cpu().detach().float().numpy())
    
    progress_bar.empty()
    status_text.text("âœ… ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å®Œäº†!")
    
    return np.vstack(all_features)

@st.cache_data
def compute_pca(features: np.ndarray, n_components: int = 10) -> Tuple[np.ndarray, PCA]:
    """
    PCAã‚’è¨ˆç®—ã™ã‚‹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰
    """
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)
    pca = PCA(n_components=n_components)
    features_pca = pca.fit_transform(features_scaled)
    
    return features_pca, pca

def create_variance_plot(pca: PCA, max_components: int = 10) -> plt.Figure:
    """
    å¯„ä¸ç‡ã®å¯è¦–åŒ–ã‚’ä½œæˆ
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    n_components = min(max_components, len(pca.explained_variance_ratio_))
    
    # å€‹åˆ¥å¯„ä¸ç‡
    ax1.bar(range(1, n_components + 1), pca.explained_variance_ratio_[:n_components])
    ax1.set_xlabel('Principal Component')
    ax1.set_ylabel('Explained Variance Ratio')
    ax1.set_title('Individual Explained Variance Ratio')
    ax1.grid(True, alpha=0.3)
    
    # ç´¯ç©å¯„ä¸ç‡
    cumsum_ratio = pca.explained_variance_ratio_[:n_components].cumsum()
    ax2.plot(range(1, n_components + 1), cumsum_ratio, 'bo-')
    ax2.axhline(y=0.8, color='r', linestyle='--', label='80%')
    ax2.axhline(y=0.9, color='g', linestyle='--', label='90%')
    ax2.axhline(y=0.95, color='orange', linestyle='--', label='95%')
    ax2.set_xlabel('Principal Component')
    ax2.set_ylabel('Cumulative Explained Variance Ratio')
    ax2.set_title('Cumulative Explained Variance Ratio')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

def create_pca_visualization(features_pca: np.ndarray, images: List[Image.Image], 
                           image_paths: List[str], encoder_type: str, pca: PCA,
                           pc_x: int = 0, pc_y: int = 1, pc_z: int = None,
                           max_images_display: int = 25, image_size: int = 60, 
                           figsize: tuple = (16, 12), save_path: Optional[str] = None) -> plt.Figure:
    """
    é¸æŠã•ã‚ŒãŸä¸»æˆåˆ†ã§PCAå¯è¦–åŒ–ã‚’ä½œæˆã™ã‚‹
    """
    # è¡¨ç¤ºã™ã‚‹ç”»åƒã‚’é¸æŠ
    if len(images) > max_images_display:
        display_indices = random.sample(range(len(images)), max_images_display)
        display_indices.sort()
    else:
        display_indices = list(range(len(images)))
    
    # matplotlibè¨­å®š
    plt.rcParams['figure.dpi'] = 150
    plt.rcParams['savefig.dpi'] = 300
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['axes.unicode_minus'] = False
    
    # å›³ã‚’ä½œæˆ
    if pc_z is None:  # 2Då¯è¦–åŒ–
        fig, ax = plt.subplots(figsize=figsize)
        
        # èƒŒæ™¯ã®æ•£å¸ƒå›³
        ax.scatter(features_pca[:, pc_x], features_pca[:, pc_y], 
                  alpha=0.3, s=15, c='lightgray', zorder=1, label='All Data Points')
        
        # é¸æŠã—ãŸç‚¹ã‚’å¼·èª¿
        selected_x = features_pca[display_indices, pc_x]
        selected_y = features_pca[display_indices, pc_y]
        ax.scatter(selected_x, selected_y, 
                  alpha=0.7, s=30, c='red', zorder=2, label='Image Display Points')
        
        # ç”»åƒé…ç½®
        for i, idx in enumerate(display_indices):
            try:
                img = images[idx].copy()
                img.thumbnail((image_size, image_size), Image.Resampling.LANCZOS)
                img_array = np.array(img)
                
                imagebox = OffsetImage(img_array, zoom=1.0)
                x_pos, y_pos = features_pca[idx, pc_x], features_pca[idx, pc_y]
                
                ab = AnnotationBbox(imagebox, (x_pos, y_pos),
                                  frameon=True, pad=0.2, zorder=3,
                                  boxcoords="data")
                ax.add_artist(ab)
                
            except Exception:
                ax.scatter(features_pca[idx, pc_x], features_pca[idx, pc_y], 
                         c='red', s=50, alpha=0.8, zorder=3, marker='x')
        
        ax.set_xlabel(f'PC{pc_x+1} (Variance Ratio: {pca.explained_variance_ratio_[pc_x]:.3f})', fontsize=14)
        ax.set_ylabel(f'PC{pc_y+1} (Variance Ratio: {pca.explained_variance_ratio_[pc_y]:.3f})', fontsize=14)
        
    else:  # 3Då¯è¦–åŒ–
        fig = plt.figure(figsize=figsize)
        ax = fig.add_subplot(111, projection='3d')
        
        ax.scatter(features_pca[:, pc_x], features_pca[:, pc_y], features_pca[:, pc_z], 
                  alpha=0.6, s=50, c='lightblue')
        ax.scatter(features_pca[display_indices, pc_x], 
                  features_pca[display_indices, pc_y], 
                  features_pca[display_indices, pc_z],
                  alpha=0.9, s=100, c='red', label='Selected Points')
        
        ax.set_xlabel(f'PC{pc_x+1} (Variance Ratio: {pca.explained_variance_ratio_[pc_x]:.3f})')
        ax.set_ylabel(f'PC{pc_y+1} (Variance Ratio: {pca.explained_variance_ratio_[pc_y]:.3f})')
        ax.set_zlabel(f'PC{pc_z+1} (Variance Ratio: {pca.explained_variance_ratio_[pc_z]:.3f})')
    
    # ã‚¿ã‚¤ãƒˆãƒ«ã¨è£…é£¾
    if pc_z is None:
        title = f'{encoder_type.upper()} Encoder PCA Visualization: PC{pc_x+1} vs PC{pc_y+1}'
        title += f'\nImage Size: {image_size}px, Count: {len(display_indices)}'
    else:
        title = f'{encoder_type.upper()} Encoder PCA Visualization: PC{pc_x+1} vs PC{pc_y+1} vs PC{pc_z+1}'
    
    plt.title(title, fontsize=16, pad=20)
    ax.grid(True, alpha=0.3)
    if pc_z is None:
        ax.legend(loc='upper right', fontsize=10)
    
    plt.tight_layout()
    
    # ä¿å­˜
    if save_path:
        save_dir = Path(save_path).parent
        save_dir.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    
    return fig

def create_pca_matrix_plot(features_pca: np.ndarray, pca: PCA, max_components: int = 6) -> plt.Figure:
    """
    PCAæ•£å¸ƒå›³è¡Œåˆ—ã‚’ä½œæˆ
    """
    n_components = min(max_components, features_pca.shape[1])
    df = pd.DataFrame(features_pca[:, :n_components], 
                     columns=[f'PC{i+1}' for i in range(n_components)])
    
    # pairplotã‚’ä½œæˆ
    g = sns.pairplot(df, diag_kind='hist', plot_kws={'alpha': 0.6, 's': 30})
    
    # è»¸ãƒ©ãƒ™ãƒ«ã«å¯„ä¸ç‡ã‚’è¿½åŠ 
    for i, ax in enumerate(g.axes[-1]):
        ax.set_xlabel(f'{ax.get_xlabel()} (Variance: {pca.explained_variance_ratio_[i]:.3f})')
    
    for i, ax in enumerate(g.axes):
        ax[0].set_ylabel(f'{ax[0].get_ylabel()} (Variance: {pca.explained_variance_ratio_[i]:.3f})')
    
    g.fig.suptitle('PCA Scatter Plot Matrix', fontsize=16, y=1.02)
    
    return g.fig

def create_interactive_pca_visualization(features_pca: np.ndarray, images: List[Image.Image], 
                                       image_paths: List[str], encoder_type: str, pca: PCA,
                                       pc_x: int = 0, pc_y: int = 1, pc_z: int = None,
                                       max_images_display: int = 50) -> go.Figure:
    """
    Plotlyã‚’ä½¿ã£ãŸã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–PCAå¯è¦–åŒ–ã‚’ä½œæˆ
    """
    # è¡¨ç¤ºã™ã‚‹ç”»åƒã‚’é¸æŠ
    if len(images) > max_images_display:
        display_indices = random.sample(range(len(images)), max_images_display)
        display_indices.sort()
    else:
        display_indices = list(range(len(images)))
    
    # ç”»åƒã‚’base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ãƒ›ãƒãƒ¼ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    hover_data = []
    image_names = []
    
    for idx in display_indices:
        try:
            img = images[idx].copy()
            img.thumbnail((150, 150), Image.Resampling.LANCZOS)
            
            # base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            buffer = BytesIO()
            img.save(buffer, format='PNG')
            img_str = base64.b64encode(buffer.getvalue()).decode()
            
            hover_data.append(f"<img src='data:image/png;base64,{img_str}' width='120'>")
            image_names.append(f"Image {idx}")
            
        except Exception as e:
            hover_data.append(f"ç”»åƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            image_names.append(f"Error {idx}")
    
    if pc_z is None:
        # 2Då¯è¦–åŒ–
        fig = go.Figure()
        
        fig.add_trace(go.Scatter(
            x=features_pca[display_indices, pc_x],
            y=features_pca[display_indices, pc_y],
            mode='markers',
            marker=dict(
                size=8,
                color='lightblue',
                line=dict(width=1, color='navy'),
                opacity=0.7
            ),
            text=[f"{image_names[i]}<br>PC{pc_x+1}: {features_pca[display_indices[i], pc_x]:.3f}<br>PC{pc_y+1}: {features_pca[display_indices[i], pc_y]:.3f}" 
                  for i in range(len(display_indices))],
            hovertemplate='%{text}<br>%{customdata}<extra></extra>',
            customdata=hover_data,
            name='Images'
        ))
        
        fig.update_layout(
            title=f'{encoder_type.upper()} ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–PCAå¯è¦–åŒ– (2D)',
            xaxis_title=f'PC{pc_x+1} (å¯„ä¸ç‡: {pca.explained_variance_ratio_[pc_x]:.3f})',
            yaxis_title=f'PC{pc_y+1} (å¯„ä¸ç‡: {pca.explained_variance_ratio_[pc_y]:.3f})',
            width=800,
            height=600,
            hovermode='closest'
        )
        
    else:
        # 3Då¯è¦–åŒ–
        fig = go.Figure()
        
        fig.add_trace(go.Scatter3d(
            x=features_pca[display_indices, pc_x],
            y=features_pca[display_indices, pc_y],
            z=features_pca[display_indices, pc_z],
            mode='markers',
            marker=dict(
                size=6,
                color='lightblue',
                line=dict(width=1, color='navy'),
                opacity=0.7
            ),
            text=[f"{image_names[i]}<br>PC{pc_x+1}: {features_pca[display_indices[i], pc_x]:.3f}<br>PC{pc_y+1}: {features_pca[display_indices[i], pc_y]:.3f}<br>PC{pc_z+1}: {features_pca[display_indices[i], pc_z]:.3f}" 
                  for i in range(len(display_indices))],
            hovertemplate='%{text}<br>%{customdata}<extra></extra>',
            customdata=hover_data,
            name='Images'
        ))
        
        fig.update_layout(
            title=f'{encoder_type.upper()} ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–PCAå¯è¦–åŒ– (3D)',
            scene=dict(
                xaxis_title=f'PC{pc_x+1} (å¯„ä¸ç‡: {pca.explained_variance_ratio_[pc_x]:.3f})',
                yaxis_title=f'PC{pc_y+1} (å¯„ä¸ç‡: {pca.explained_variance_ratio_[pc_y]:.3f})',
                zaxis_title=f'PC{pc_z+1} (å¯„ä¸ç‡: {pca.explained_variance_ratio_[pc_z]:.3f})',
            ),
            width=800,
            height=600
        )
    
    return fig

def create_multi_dimension_comparison(features_pca: np.ndarray, pca: PCA, 
                                    max_components: int = 6) -> go.Figure:
    """
    è¤‡æ•°æ¬¡å…ƒã®æ¯”è¼ƒå¯è¦–åŒ–ã‚’ä½œæˆ
    """
    n_components = min(max_components, features_pca.shape[1])
    
    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ
    dimension_pairs = [(i, j) for i in range(n_components) for j in range(i+1, n_components)][:6]
    
    fig = make_subplots(
        rows=2, cols=3,
        subplot_titles=[f'PC{pair[0]+1} vs PC{pair[1]+1}' for pair in dimension_pairs],
        specs=[[{'type': 'scatter'} for _ in range(3)] for _ in range(2)]
    )
    
    for idx, (pc_x, pc_y) in enumerate(dimension_pairs):
        row = idx // 3 + 1
        col = idx % 3 + 1
        
        fig.add_trace(
            go.Scatter(
                x=features_pca[:, pc_x],
                y=features_pca[:, pc_y],
                mode='markers',
                marker=dict(size=4, opacity=0.6),
                name=f'PC{pc_x+1} vs PC{pc_y+1}',
                showlegend=False
            ),
            row=row, col=col
        )
        
        fig.update_xaxes(title_text=f'PC{pc_x+1} ({pca.explained_variance_ratio_[pc_x]:.3f})', row=row, col=col)
        fig.update_yaxes(title_text=f'PC{pc_y+1} ({pca.explained_variance_ratio_[pc_y]:.3f})', row=row, col=col)
    
    fig.update_layout(
        title_text="è¤‡æ•°æ¬¡å…ƒPCAæ¯”è¼ƒ",
        height=600,
        width=900
    )
    
    return fig

def get_default_checkpoint_paths():
    """
    å„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‘ã‚¹ã‚’å–å¾—
    """
    return {
        'clip': "openai/clip-vit-large-patch14",  # Hugging Face ID
        'emotion_clip': "/home/ryuichi/animins/slider_space_v2/EmotionCLIP/emotionclip_latest.pt",
        'dinov2-small': "facebook/dinov2-small",  # Hugging Face ID
        'openvision': "/home/ryuichi/animins/slider_space_v2/models/openvision_checkpoint.pt"
    }

def find_checkpoint_candidates(encoder_type: str):
    """
    æŒ‡å®šã•ã‚ŒãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®å€™è£œãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æ¤œç´¢
    """
    candidates = []
    
    if encoder_type == 'emotion_clip':
        search_dirs = [
            "/home/ryuichi/animins/slider_space_v2/EmotionCLIP/",
            "/home/ryuichi/animins/EmotionCLIP/",
            "/home/ryuichi/EmotionCLIP/",
            str(Path.home() / "EmotionCLIP"),
            "./EmotionCLIP/",
            "../EmotionCLIP/"
        ]
        file_patterns = ["*.pt", "*.pth", "*.ckpt"]
        
    elif encoder_type == 'openvision':
        search_dirs = [
            "/home/ryuichi/animins/slider_space_v2/models/",
            "/home/ryuichi/animins/models/",
            str(Path.home() / "models"),
            "./models/",
            "../models/"
        ]
        file_patterns = ["*.pt", "*.pth", "*.ckpt"]
        
    elif encoder_type in ['clip', 'dinov2-small']:
        # Hugging Faceãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚‚æ¤œç´¢
        cache_dirs = [
            str(Path.home() / ".cache/huggingface/transformers/"),
            str(Path.home() / ".cache/huggingface/hub/"),
            "/tmp/huggingface_cache/"
        ]
        search_dirs = cache_dirs
        file_patterns = ["*.bin", "*.safetensors", "*.pt"]
        
    else:
        return []
    
    # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
    for search_dir in search_dirs:
        if os.path.exists(search_dir):
            for pattern in file_patterns:
                for file_path in Path(search_dir).glob(pattern):
                    if file_path.is_file():
                        candidates.append(str(file_path))
    
    # é‡è¤‡é™¤å»ã¨ã‚½ãƒ¼ãƒˆ
    return sorted(list(set(candidates)))

def create_checkpoint_selector(encoder_type: str, key_prefix: str = ""):
    """
    ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé¸æŠUIã‚’ä½œæˆ
    """
    default_paths = get_default_checkpoint_paths()
    default_path = default_paths.get(encoder_type, "")
    
    st.sidebar.subheader(f"ğŸ“ {encoder_type.upper()} ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆè¨­å®š")
    
    # é¸æŠæ–¹æ³•
    selection_methods = ["ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨", "ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¹æŒ‡å®š", "ãƒ•ã‚¡ã‚¤ãƒ«ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¼"]
    
    # Hugging Faceãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯èª¬æ˜ã‚’è¿½åŠ 
    if encoder_type in ['clip', 'dinov2-small']:
        selection_methods[0] = "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨ (Hugging Face)"
    
    checkpoint_method = st.sidebar.radio(
        f"{encoder_type.upper()} é¸æŠæ–¹æ³•",
        selection_methods,
        index=0,
        key=f"{key_prefix}checkpoint_method_{encoder_type}"
    )
    
    if checkpoint_method.startswith("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨"):
        checkpoint_path = default_path
        if encoder_type in ['clip', 'dinov2-small']:
            st.sidebar.info(f"ğŸ¤— Hugging Face ID: `{checkpoint_path}`")
        else:
            st.sidebar.info(f"ğŸ“ ä½¿ç”¨ãƒ‘ã‚¹: `{checkpoint_path}`")
            
    elif checkpoint_method == "ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¹æŒ‡å®š":
        help_text = "ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ãƒ«ãƒ‘ã‚¹ã¾ãŸã¯Hugging Face IDã‚’å…¥åŠ›"
        if encoder_type in ['clip', 'dinov2-small']:
            help_text += "\nä¾‹: openai/clip-vit-base-patch32 ã¾ãŸã¯ /path/to/model"
        
        checkpoint_path = st.sidebar.text_input(
            f"{encoder_type.upper()} ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‘ã‚¹",
            value=default_path,
            help=help_text,
            key=f"{key_prefix}custom_path_{encoder_type}"
        )
        
    else:  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¼
        candidates = find_checkpoint_candidates(encoder_type)
        
        if candidates:
            # å€™è£œãƒªã‚¹ãƒˆã‚’ä½œæˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã‚«ã‚¹ã‚¿ãƒ ã‚‚å«ã‚€ï¼‰
            all_options = [
                f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {default_path}",
                "ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¹æŒ‡å®š..."
            ] + [f"ãƒ­ãƒ¼ã‚«ãƒ«: {Path(p).name} ({p})" for p in candidates[:10]]  # æœ€å¤§10å€‹ã¾ã§è¡¨ç¤º
            
            if len(candidates) > 10:
                all_options.append(f"... ä»– {len(candidates) - 10} å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«")
            
            selected_option = st.sidebar.selectbox(
                f"{encoder_type.upper()} ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ",
                all_options,
                index=0,
                help="åˆ©ç”¨å¯èƒ½ãªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰é¸æŠ",
                key=f"{key_prefix}file_browser_{encoder_type}"
            )
            
            if selected_option.startswith("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ:"):
                checkpoint_path = default_path
            elif selected_option == "ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¹æŒ‡å®š...":
                checkpoint_path = st.sidebar.text_input(
                    "ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¹",
                    value=default_path,
                    key=f"{key_prefix}custom_fallback_{encoder_type}"
                )
            elif selected_option.startswith("ãƒ­ãƒ¼ã‚«ãƒ«:"):
                # "ãƒ­ãƒ¼ã‚«ãƒ«: filename (full_path)" ã‹ã‚‰ full_path ã‚’æŠ½å‡º
                checkpoint_path = selected_option.split("(")[-1].rstrip(")")
            else:
                checkpoint_path = default_path
        else:
            st.sidebar.warning(f"âš ï¸ {encoder_type.upper()} ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            checkpoint_path = st.sidebar.text_input(
                f"{encoder_type.upper()} ãƒ‘ã‚¹ï¼ˆæ‰‹å‹•å…¥åŠ›ï¼‰",
                value=default_path,
                key=f"{key_prefix}manual_input_{encoder_type}"
            )
    
    # ãƒ•ã‚¡ã‚¤ãƒ«/ID ã®æ¤œè¨¼ã¨æƒ…å ±è¡¨ç¤º
    if checkpoint_path:
        if checkpoint_path.startswith(("openai/", "facebook/", "microsoft/", "google/")):
            # Hugging Face ID ã®å ´åˆ
            st.sidebar.success(f"ğŸ¤— Hugging Face ãƒ¢ãƒ‡ãƒ« ID: `{checkpoint_path}`")
        elif os.path.exists(checkpoint_path):
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
            file_size = os.path.getsize(checkpoint_path) / (1024 * 1024)  # MB
            st.sidebar.success(f"âœ… ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªæ¸ˆã¿ ({file_size:.1f} MB)")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°æƒ…å ±
            with st.sidebar.expander(f"ğŸ“Š {encoder_type.upper()} ãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°"):
                file_stat = os.stat(checkpoint_path)
                st.write(f"**ãƒ‘ã‚¹**: `{checkpoint_path}`")
                st.write(f"**ãƒ•ã‚¡ã‚¤ãƒ«å**: `{Path(checkpoint_path).name}`")
                st.write(f"**ã‚µã‚¤ã‚º**: {file_size:.2f} MB")
                st.write(f"**æ›´æ–°æ—¥æ™‚**: {time.ctime(file_stat.st_mtime)}")
        else:
            # å­˜åœ¨ã—ãªã„ãƒ‘ã‚¹ã®å ´åˆ
            if "/" in checkpoint_path and not checkpoint_path.startswith(("openai/", "facebook/")):
                st.sidebar.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: `{checkpoint_path}`")
            else:
                st.sidebar.warning(f"âš ï¸ Hugging Face IDã¾ãŸã¯ãƒ‘ã‚¹ã‚’ç¢ºèª: `{checkpoint_path}`")
    
    return checkpoint_path

def main():
    # ã‚¿ã‚¤ãƒˆãƒ«
    st.title("ğŸ¨ æ‹¡å¼µç‰ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰PCAå¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("### ä¸»æˆåˆ†é¸æŠæ©Ÿèƒ½ä»˜ãã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å¯è¦–åŒ–ãƒ„ãƒ¼ãƒ«")
    
    # åˆ©ç”¨å¯èƒ½ãªæ©Ÿèƒ½ã®è¡¨ç¤º
    with st.expander("ğŸ”§ åˆ©ç”¨å¯èƒ½ãªæ©Ÿèƒ½ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæƒ…å ±", expanded=False):
        col1, col2, col3 = st.columns(3)
        with col1:
            st.write("**âœ… åŸºæœ¬ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼**")
            
            # CLIPæƒ…å ±
            clip_checkpoint = getattr(st.session_state, 'clip_checkpoint', 'openai/clip-vit-large-patch14')
            st.write("â€¢ CLIP")
            if clip_checkpoint.startswith('openai/'):
                st.write(f"  â”” ğŸ¤— {Path(clip_checkpoint).name}")
            else:
                st.write(f"  â”” ğŸ“ {Path(clip_checkpoint).name}")
            
            # DINOv2æƒ…å ±
            dinov2_checkpoint = getattr(st.session_state, 'dinov2-small_checkpoint', 'facebook/dinov2-small')
            st.write("â€¢ DINOv2")
            if dinov2_checkpoint.startswith('facebook/'):
                st.write(f"  â”” ğŸ¤— {Path(dinov2_checkpoint).name}")
            else:
                st.write(f"  â”” ğŸ“ {Path(dinov2_checkpoint).name}")
        
        with col2:
            st.write("**ğŸ”„ ã‚ªãƒ—ã‚·ãƒ§ãƒ³æ©Ÿèƒ½**")
            
            if EMOTION_CLIP_AVAILABLE:
                st.write("âœ… EmotionCLIP")
                emotion_checkpoint = getattr(st.session_state, 'emotion_clip_checkpoint', '')
                if emotion_checkpoint:
                    st.write(f"  â”” ğŸ“ {Path(emotion_checkpoint).name}")
            else:
                st.write("âŒ EmotionCLIP")
            
            if OPENVISION_AVAILABLE:
                st.write("âœ… OpenVision")
                openvision_checkpoint = getattr(st.session_state, 'openvision_checkpoint', '')
                if openvision_checkpoint:
                    if openvision_checkpoint.startswith(('microsoft/', 'google/')):
                        st.write(f"  â”” ğŸ¤— {Path(openvision_checkpoint).name}")
                    else:
                        st.write(f"  â”” ğŸ“ {Path(openvision_checkpoint).name}")
            else:
                st.write("âŒ OpenVision")
        
        with col3:
            st.write("**ğŸ¨ å¯è¦–åŒ–æ©Ÿèƒ½**")
            st.write("âœ… matplotlibå¯è¦–åŒ–")
            st.write("âœ… Plotly ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–")
            st.write("âœ… è¤‡æ•°æ¬¡å…ƒæ¯”è¼ƒ")
            st.write("âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé¸æŠ")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    st.sidebar.header("ğŸ“‹ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š")
    
    # ãƒ‡ãƒ¼ã‚¿è¨­å®š
    st.sidebar.subheader("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿è¨­å®š")
    data_path = st.sidebar.text_input(
        "ç”»åƒãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹", 
        value="/home/ryuichi/animins/slider_space_v2/training_images/sdxl/concept_15109"
    )
    max_images = st.sidebar.slider("æœ€å¤§ç”»åƒæ•°", 5, 200, 50, 5)
    
    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼è¨­å®š
    st.sidebar.subheader("ğŸ¤– ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼è¨­å®š")
    available_encoders = ['clip', 'dinov2-small']
    encoder_descriptions = {
        'clip': 'CLIP (é«˜å“è³ªãƒ»æ±ç”¨)',
        'dinov2-small': 'DINOv2 (è¦–è¦šç‰¹å¾´ç‰¹åŒ–)'
    }
    
    if EMOTION_CLIP_AVAILABLE:
        available_encoders.insert(1, 'emotion_clip')
        encoder_descriptions['emotion_clip'] = 'EmotionCLIP (æ„Ÿæƒ…ç†è§£)'
    
    if OPENVISION_AVAILABLE:
        available_encoders.append('openvision')
        encoder_descriptions['openvision'] = 'OpenVision (ã‚«ã‚¹ã‚¿ãƒ )'
    
    encoder_type = st.sidebar.selectbox(
        "ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼", 
        available_encoders, 
        format_func=lambda x: encoder_descriptions.get(x, x),
        index=1 if EMOTION_CLIP_AVAILABLE else 0
    )
    
    # é¸æŠã•ã‚ŒãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆè¨­å®š
    checkpoint_path = create_checkpoint_selector(encoder_type, "main_")
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
    st.session_state[f'{encoder_type}_checkpoint'] = checkpoint_path
    
    batch_size = st.sidebar.slider("ãƒãƒƒãƒã‚µã‚¤ã‚º", 1, 16, 8, 1)
    
    # PCAè¨­å®š
    st.sidebar.subheader("ğŸ“Š PCAè¨­å®š")
    max_pca_components = st.sidebar.slider("æœ€å¤§PCAæˆåˆ†æ•°", 3, 20, 10, 1)
    
    # å¯è¦–åŒ–è¨­å®š
    st.sidebar.subheader("ğŸ¨ å¯è¦–åŒ–è¨­å®š")
    max_images_display = st.sidebar.slider("è¡¨ç¤ºç”»åƒæ•°", 5, 50, 25, 1)
    image_size = st.sidebar.slider("ç”»åƒã‚µã‚¤ã‚º (px)", 30, 120, 60, 10)
    figsize_width = st.sidebar.slider("å›³ã®å¹…", 10, 20, 16, 1)
    figsize_height = st.sidebar.slider("å›³ã®é«˜ã•", 8, 16, 12, 1)
    
    # ä¿å­˜è¨­å®š
    st.sidebar.subheader("ğŸ’¾ ä¿å­˜è¨­å®š")
    save_results = st.sidebar.checkbox("çµæœã‚’ä¿å­˜", value=True)
    save_path = st.sidebar.text_input(
        "ä¿å­˜ãƒ‘ã‚¹", 
        value="/home/ryuichi/animins/slider_space_v2/pca_visualization/enhanced_ui_result.png"
    ) if save_results else None
    
    # å®Ÿè¡Œãƒœã‚¿ãƒ³
    if st.sidebar.button("ğŸš€ å¯è¦–åŒ–å®Ÿè¡Œ", type="primary"):
        try:
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            with st.spinner("ğŸ“‚ ç”»åƒãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­..."):
                images, image_paths = load_image_dataset(data_path, max_images)
            
            if len(images) == 0:
                st.error("âŒ ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return
            
            st.success(f"âœ… {len(images)}æšã®ç”»åƒã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            
            # ç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            st.subheader("ğŸ“¸ èª­ã¿è¾¼ã¿ç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            preview_cols = st.columns(6)
            for i, img in enumerate(images[:6]):
                with preview_cols[i % 6]:
                    st.image(img, caption=f"Image {i+1}", use_column_width=True)
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            with st.spinner(f"ğŸ”„ {encoder_type.upper()}ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ä¸­..."):
                features = encode_images(images, encoder_type, batch_size)
            
            st.success(f"âœ… ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å®Œäº†: ç‰¹å¾´é‡ã®å½¢çŠ¶ {features.shape}")
            
            # PCAè¨ˆç®—
            with st.spinner("ğŸ“Š PCAè¨ˆç®—ä¸­..."):
                features_pca, pca = compute_pca(features, max_pca_components)
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
            st.session_state.features_pca = features_pca
            st.session_state.pca = pca
            st.session_state.images = images
            st.session_state.image_paths = image_paths
            st.session_state.encoder_type = encoder_type
            st.session_state.max_images_display = max_images_display
            st.session_state.image_size = image_size
            st.session_state.figsize = (figsize_width, figsize_height)
            st.session_state.save_path = save_path
            
            st.success(f"âœ… PCAè¨ˆç®—å®Œäº†: {features_pca.shape[1]}å€‹ã®ä¸»æˆåˆ†ã‚’å–å¾—")
            
        except Exception as e:
            st.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            st.exception(e)
    
    # PCAãŒè¨ˆç®—ã•ã‚Œã¦ã„ã‚‹å ´åˆã®è¿½åŠ UI
    if hasattr(st.session_state, 'features_pca') and st.session_state.features_pca is not None:
        st.markdown("---")
        st.header("ğŸ“Š PCAçµæœåˆ†æ")
        
        # ã‚¿ãƒ–ã§æ©Ÿèƒ½ã‚’åˆ†å‰²
        tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
            "ğŸ¯ ä¸»æˆåˆ†é¸æŠå¯è¦–åŒ–", 
            "ğŸš€ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å¯è¦–åŒ–", 
            "ğŸ“Š è¤‡æ•°æ¬¡å…ƒæ¯”è¼ƒ",
            "ğŸ“ˆ å¯„ä¸ç‡åˆ†æ", 
            "ğŸ” æ•£å¸ƒå›³è¡Œåˆ—", 
            "ğŸ“‹ çµ±è¨ˆæƒ…å ±"
        ])
        
        with tab1:
            st.subheader("ğŸ¯ ä¸»æˆåˆ†é¸æŠå¯è¦–åŒ–")
            st.markdown("**ä»»æ„ã®ä¸»æˆåˆ†ã®çµ„ã¿åˆã‚ã›ã§å¯è¦–åŒ–ã§ãã¾ã™**")
            
            # ä¸»æˆåˆ†é¸æŠ
            col1, col2, col3 = st.columns(3)
            
            n_components = st.session_state.features_pca.shape[1]
            component_options = list(range(1, n_components + 1))
            
            with col1:
                pc_x = st.selectbox("Xè»¸ (PC)", component_options, index=0, key="pc_x") - 1
            
            with col2:
                pc_y = st.selectbox("Yè»¸ (PC)", component_options, index=1, key="pc_y") - 1
            
            with col3:
                use_3d = st.checkbox("3Dè¡¨ç¤º", key="use_3d")
                if use_3d:
                    pc_z = st.selectbox("Zè»¸ (PC)", component_options, index=2, key="pc_z") - 1
                else:
                    pc_z = None
            
            # å¯è¦–åŒ–å®Ÿè¡Œ
            if st.button("ğŸ”„ å¯è¦–åŒ–æ›´æ–°", key="update_viz"):
                with st.spinner("ğŸ¨ å¯è¦–åŒ–ä½œæˆä¸­..."):
                    fig = create_pca_visualization(
                        st.session_state.features_pca, st.session_state.images, 
                        st.session_state.image_paths, st.session_state.encoder_type,
                        st.session_state.pca, pc_x, pc_y, pc_z,
                        st.session_state.max_images_display, st.session_state.image_size,
                        st.session_state.figsize, st.session_state.save_path
                    )
                
                st.pyplot(fig)
                
                # é¸æŠã•ã‚ŒãŸä¸»æˆåˆ†ã®æƒ…å ±è¡¨ç¤º
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric(f"PC{pc_x+1} å¯„ä¸ç‡", f"{st.session_state.pca.explained_variance_ratio_[pc_x]:.4f}")
                with col2:
                    st.metric(f"PC{pc_y+1} å¯„ä¸ç‡", f"{st.session_state.pca.explained_variance_ratio_[pc_y]:.4f}")
                if pc_z is not None:
                    with col3:
                        st.metric(f"PC{pc_z+1} å¯„ä¸ç‡", f"{st.session_state.pca.explained_variance_ratio_[pc_z]:.4f}")
                
                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
                buf = io.BytesIO()
                fig.savefig(buf, format='png', dpi=300, bbox_inches='tight')
                buf.seek(0)
                
                filename_suffix = f"PC{pc_x+1}_vs_PC{pc_y+1}"
                if pc_z is not None:
                    filename_suffix += f"_vs_PC{pc_z+1}"
                
                st.download_button(
                    label="ğŸ“¥ PNGç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=buf.getvalue(),
                    file_name=f"pca_visualization_{st.session_state.encoder_type}_{filename_suffix}_{int(time.time())}.png",
                    mime="image/png"
                )
                
                plt.close(fig)
        
        with tab2:
            st.subheader("ğŸš€ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–PCAå¯è¦–åŒ–")
            st.markdown("**Plotlyã‚’ä½¿ã£ãŸé«˜åº¦ãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å¯è¦–åŒ–ï¼ˆãƒ›ãƒãƒ¼ã§ç”»åƒè¡¨ç¤ºï¼‰**")
            
            # ä¸»æˆåˆ†é¸æŠ
            col1, col2, col3 = st.columns(3)
            
            n_components = st.session_state.features_pca.shape[1]
            component_options = list(range(1, n_components + 1))
            
            with col1:
                interactive_pc_x = st.selectbox("Xè»¸ (PC)", component_options, index=0, key="interactive_pc_x") - 1
            
            with col2:
                interactive_pc_y = st.selectbox("Yè»¸ (PC)", component_options, index=1, key="interactive_pc_y") - 1
            
            with col3:
                interactive_use_3d = st.checkbox("3Dè¡¨ç¤º", key="interactive_use_3d")
                if interactive_use_3d:
                    interactive_pc_z = st.selectbox("Zè»¸ (PC)", component_options, index=2, key="interactive_pc_z") - 1
                else:
                    interactive_pc_z = None
            
            # è¡¨ç¤ºè¨­å®š
            interactive_max_display = st.slider(
                "è¡¨ç¤ºç”»åƒæ•°ï¼ˆã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ï¼‰", 
                min_value=10, 
                max_value=min(100, len(st.session_state.images)), 
                value=min(30, len(st.session_state.images)),
                key="interactive_max_display"
            )
            
            if st.button("ğŸ”„ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å¯è¦–åŒ–ç”Ÿæˆ", key="generate_interactive"):
                with st.spinner("ğŸ¨ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å¯è¦–åŒ–ç”Ÿæˆä¸­..."):
                    interactive_fig = create_interactive_pca_visualization(
                        st.session_state.features_pca, 
                        st.session_state.images, 
                        st.session_state.image_paths, 
                        st.session_state.encoder_type,
                        st.session_state.pca, 
                        interactive_pc_x, 
                        interactive_pc_y, 
                        interactive_pc_z,
                        interactive_max_display
                    )
                
                st.plotly_chart(interactive_fig, use_container_width=True)
                
                # HTMLãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                interactive_html = interactive_fig.to_html()
                st.download_button(
                    label="ğŸ“¥ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–HTMLã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=interactive_html,
                    file_name=f"interactive_pca_{st.session_state.encoder_type}_{int(time.time())}.html",
                    mime="text/html"
                )
        
        with tab3:
            st.subheader("ğŸ“Š è¤‡æ•°æ¬¡å…ƒæ¯”è¼ƒ")
            st.markdown("**è¤‡æ•°ã®ä¸»æˆåˆ†ãƒšã‚¢ã‚’åŒæ™‚ã«æ¯”è¼ƒè¡¨ç¤º**")
            
            comparison_components = st.slider(
                "æ¯”è¼ƒã™ã‚‹ä¸»æˆåˆ†æ•°", 
                min_value=3, 
                max_value=min(8, st.session_state.features_pca.shape[1]), 
                value=min(6, st.session_state.features_pca.shape[1]),
                key="comparison_components"
            )
            
            if st.button("ğŸ“Š è¤‡æ•°æ¬¡å…ƒæ¯”è¼ƒç”Ÿæˆ", key="generate_comparison"):
                with st.spinner("ğŸ“Š è¤‡æ•°æ¬¡å…ƒæ¯”è¼ƒç”Ÿæˆä¸­..."):
                    comparison_fig = create_multi_dimension_comparison(
                        st.session_state.features_pca, 
                        st.session_state.pca, 
                        comparison_components
                    )
                
                st.plotly_chart(comparison_fig, use_container_width=True)
                
                # HTMLãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                comparison_html = comparison_fig.to_html()
                st.download_button(
                    label="ğŸ“¥ æ¯”è¼ƒå›³HTMLã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=comparison_html,
                    file_name=f"multi_comparison_pca_{st.session_state.encoder_type}_{int(time.time())}.html",
                    mime="text/html"
                )
        
        with tab4:
            st.subheader("ğŸ“ˆ å¯„ä¸ç‡åˆ†æ")
            
            # å¯„ä¸ç‡å¯è¦–åŒ–
            variance_fig = create_variance_plot(st.session_state.pca, max_pca_components)
            st.pyplot(variance_fig)
            
            # å¯„ä¸ç‡ãƒ†ãƒ¼ãƒ–ãƒ«
            st.subheader("ğŸ“‹ ä¸»æˆåˆ†åˆ¥å¯„ä¸ç‡")
            variance_data = []
            cumsum = 0
            for i, ratio in enumerate(st.session_state.pca.explained_variance_ratio_):
                cumsum += ratio
                variance_data.append({
                    "ä¸»æˆåˆ†": f"PC{i+1}",
                    "å¯„ä¸ç‡": f"{ratio:.4f}",
                    "å¯„ä¸ç‡(%)": f"{ratio*100:.2f}%",
                    "ç´¯ç©å¯„ä¸ç‡": f"{cumsum:.4f}",
                    "ç´¯ç©å¯„ä¸ç‡(%)": f"{cumsum*100:.2f}%"
                })
            
            st.dataframe(pd.DataFrame(variance_data), use_container_width=True)
            
            plt.close(variance_fig)
        
        with tab5:
            st.subheader("ğŸ” PCAæ•£å¸ƒå›³è¡Œåˆ—")
            
            matrix_components = st.slider("è¡¨ç¤ºã™ã‚‹ä¸»æˆåˆ†æ•°", 3, min(8, n_components), min(6, n_components))
            
            if st.button("ğŸ“Š æ•£å¸ƒå›³è¡Œåˆ—ä½œæˆ", key="create_matrix"):
                with st.spinner("ğŸ” æ•£å¸ƒå›³è¡Œåˆ—ä½œæˆä¸­..."):
                    matrix_fig = create_pca_matrix_plot(st.session_state.features_pca, 
                                                      st.session_state.pca, matrix_components)
                
                st.pyplot(matrix_fig)
                plt.close(matrix_fig)
        
        with tab6:
            st.subheader("ğŸ“‹ çµ±è¨ˆæƒ…å ±")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼", st.session_state.encoder_type.upper())
                st.metric("èª­ã¿è¾¼ã¿ç”»åƒæ•°", len(st.session_state.images))
                st.metric("ç‰¹å¾´é‡æ¬¡å…ƒ", st.session_state.features_pca.shape[0])
                st.metric("PCAæˆåˆ†æ•°", st.session_state.features_pca.shape[1])
            
            with col2:
                st.metric("PC1-PC2 ç´¯ç©å¯„ä¸ç‡", f"{st.session_state.pca.explained_variance_ratio_[:2].sum():.4f}")
                st.metric("PC1-PC3 ç´¯ç©å¯„ä¸ç‡", f"{st.session_state.pca.explained_variance_ratio_[:3].sum():.4f}")
                st.metric("å…¨ä½“ç´¯ç©å¯„ä¸ç‡", f"{st.session_state.pca.explained_variance_ratio_.sum():.4f}")
                st.metric("è¡¨ç¤ºç”»åƒæ•°", st.session_state.max_images_display)
    
    # ä½¿ç”¨æ–¹æ³•
    with st.expander("ğŸ“– ä½¿ç”¨æ–¹æ³•"):
        st.markdown("""
        ### ğŸš€ æ‹¡å¼µç‰ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰PCAå¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®ä½¿ã„æ–¹
        
        #### 1. åŸºæœ¬è¨­å®š
        - **ğŸ“‚ ãƒ‡ãƒ¼ã‚¿è¨­å®š**: ç”»åƒãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹ã¨æœ€å¤§ç”»åƒæ•°ã‚’è¨­å®š
        - **ğŸ¤– ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼é¸æŠ**: CLIPã€EmotionCLIPã€DINOv2ã€OpenVisionã‹ã‚‰é¸æŠ
        - **ğŸ“ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆè¨­å®š**: å„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã§3ã¤ã®æ–¹æ³•ã‹ã‚‰é¸æŠ
          - **ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨**: æ¨å¥¨ãƒ‘ã‚¹/Hugging Face IDã‚’è‡ªå‹•ä½¿ç”¨
          - **ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¹æŒ‡å®š**: ä»»æ„ã®ãƒ‘ã‚¹ã¾ãŸã¯Hugging Face IDã‚’æ‰‹å‹•å…¥åŠ›
          - **ãƒ•ã‚¡ã‚¤ãƒ«ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¼**: ã‚·ã‚¹ãƒ†ãƒ ãŒè‡ªå‹•æ¤œç´¢ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰é¸æŠ
        - **ğŸ“Š PCAè¨­å®š**: æœ€å¤§PCAæˆåˆ†æ•°ã‚’è¨­å®šï¼ˆ3-20ï¼‰
        - **ğŸ¨ å¯è¦–åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: ç”»åƒã‚µã‚¤ã‚ºã€è¡¨ç¤ºæ•°ã€å›³ã®ã‚µã‚¤ã‚ºã‚’èª¿æ•´
        - **ğŸš€ å®Ÿè¡Œ**: ã€Œå¯è¦–åŒ–å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
        
        #### 2. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé¸æŠã®è©³ç´° ğŸ“
        
        **CLIP & DINOv2**:
        - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Hugging Faceå…¬å¼ãƒ¢ãƒ‡ãƒ«ï¼ˆè‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰
        - ã‚«ã‚¹ã‚¿ãƒ : ç‹¬è‡ªã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯ä»–ã®Hugging Face ID
        - ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¼: ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰é¸æŠ
        
        **EmotionCLIP**:
        - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®æ¨™æº–ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        - ã‚«ã‚¹ã‚¿ãƒ : ä»»æ„ã®.ptãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        - ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¼: è¤‡æ•°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰.ptãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•æ¤œç´¢
        
        **OpenVision**:
        - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®æ¨™æº–ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        - ã‚«ã‚¹ã‚¿ãƒ : ä»»æ„ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        - ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¼: modelsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰è‡ªå‹•æ¤œç´¢
        
        #### 3. ä¸»æˆåˆ†é¸æŠå¯è¦–åŒ– ğŸ¯
        - **Xè»¸ãƒ»Yè»¸**: ä»»æ„ã®ä¸»æˆåˆ†ã‚’é¸æŠï¼ˆPC1, PC2, PC3...ï¼‰
        - **3Dè¡¨ç¤º**: ãƒã‚§ãƒƒã‚¯ã§Zè»¸ã‚‚é¸æŠå¯èƒ½
        - **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¤‰æ›´å¾Œã€Œå¯è¦–åŒ–æ›´æ–°ã€ã§å³åº§ã«åæ˜ 
        
        #### 4. åˆ†ææ©Ÿèƒ½
        - **ğŸ“ˆ å¯„ä¸ç‡åˆ†æ**: å„ä¸»æˆåˆ†ã®é‡è¦åº¦ã‚’ç¢ºèª
        - **ğŸ” æ•£å¸ƒå›³è¡Œåˆ—**: è¤‡æ•°ä¸»æˆåˆ†ã®é–¢ä¿‚ã‚’ä¸€åº¦ã«è¡¨ç¤º
        - **ğŸ“‹ çµ±è¨ˆæƒ…å ±**: è©³ç´°ãªæ•°å€¤ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
        
        ### âœ¨ æ–°æ©Ÿèƒ½
        - **ğŸ¯ ä»»æ„ä¸»æˆåˆ†é¸æŠ**: PC1 vs PC3ã€PC2 vs PC4ãªã©è‡ªç”±ãªçµ„ã¿åˆã‚ã›
        - **ğŸ“ å…¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé¸æŠ**: å…¨ã¦ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã§ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½
        - **ğŸ” è‡ªå‹•ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢**: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•æ¤œå‡ºã¨ãƒªã‚¹ãƒˆè¡¨ç¤º
        - **ğŸ“Š è©³ç´°åˆ†æ**: å¯„ä¸ç‡ãƒ†ãƒ¼ãƒ–ãƒ«ã€æ•£å¸ƒå›³è¡Œåˆ—
        - **ğŸ”„ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¤‰æ›´ã§å³åº§ã«å†ç”Ÿæˆ
        - **ğŸ’¾ é«˜åº¦ãªä¿å­˜**: ä¸»æˆåˆ†æƒ…å ±ä»˜ããƒ•ã‚¡ã‚¤ãƒ«åã§è‡ªå‹•ä¿å­˜
        
        ### ğŸ›¡ï¸ å®‰å…¨æ©Ÿèƒ½
        - **ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª**: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•æ¤œè¨¼
        - **è©³ç´°æƒ…å ±è¡¨ç¤º**: ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã€æ›´æ–°æ—¥æ™‚ãªã©ã®è©³ç´°æƒ…å ±
        - **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: ã‚ã‹ã‚Šã‚„ã™ã„ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨è§£æ±ºç­–æç¤º
        - **å€™è£œãƒ•ã‚¡ã‚¤ãƒ«ææ¡ˆ**: ã‚¨ãƒ©ãƒ¼æ™‚ã«åˆ©ç”¨å¯èƒ½ãªä»£æ›¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•ææ¡ˆ
        """)
    
    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown("---")
    st.markdown("ğŸ¨ **æ‹¡å¼µç‰ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰PCAå¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ ** - ä¸»æˆåˆ†é¸æŠæ©Ÿèƒ½ä»˜ã")

if __name__ == "__main__":
    main()